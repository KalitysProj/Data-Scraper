# ğŸš€ Tutoriel Complet - Backend INPI Scraper

Ce tutoriel vous guide pas Ã  pas pour installer et configurer le backend complet du scraper INPI.

## ğŸ“‹ PrÃ©requis

### 1. Logiciels requis
- **Node.js** (version 18+) : [TÃ©lÃ©charger](https://nodejs.org/)
- **MySQL** (version 8+) : [TÃ©lÃ©charger](https://dev.mysql.com/downloads/)
- **Git** : [TÃ©lÃ©charger](https://git-scm.com/)

### 2. VÃ©rification des installations
```bash
node --version    # Doit afficher v18+ 
npm --version     # Doit afficher 9+
mysql --version   # Doit afficher 8+
```

## ğŸ› ï¸ Installation Backend

### Ã‰tape 1: CrÃ©er le projet backend
```bash
# CrÃ©er le dossier backend
mkdir inpi-scraper-backend
cd inpi-scraper-backend

# Initialiser le projet Node.js
npm init -y
```

### Ã‰tape 2: Installer les dÃ©pendances
```bash
# DÃ©pendances principales
npm install express mysql2 puppeteer cors helmet express-rate-limit dotenv bcryptjs jsonwebtoken express-validator winston bull redis csv-writer uuid

# DÃ©pendances de dÃ©veloppement
npm install --save-dev nodemon jest supertest
```

### Ã‰tape 3: CrÃ©er la structure des dossiers
```bash
mkdir -p src/{config,controllers,middleware,routes,services,utils,database}
mkdir -p logs temp
```

## ğŸ—„ï¸ Configuration Base de DonnÃ©es

### Ã‰tape 1: CrÃ©er la base de donnÃ©es MySQL
```sql
-- Se connecter Ã  MySQL en tant qu'administrateur
mysql -u root -p

-- CrÃ©er la base de donnÃ©es
CREATE DATABASE inpi_scraper CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- CrÃ©er un utilisateur dÃ©diÃ© (optionnel mais recommandÃ©)
CREATE USER 'inpi_user'@'localhost' IDENTIFIED BY 'votre_mot_de_passe_securise';
GRANT ALL PRIVILEGES ON inpi_scraper.* TO 'inpi_user'@'localhost';
FLUSH PRIVILEGES;

-- Quitter MySQL
EXIT;
```

### Ã‰tape 2: Configuration des variables d'environnement
```bash
# Copier le fichier d'exemple
cp .env.example .env

# Ã‰diter le fichier .env avec vos paramÃ¨tres
nano .env
```

**Contenu du fichier .env :**
```env
# Configuration de la base de donnÃ©es
DB_HOST=localhost
DB_PORT=3306
DB_NAME=inpi_scraper
DB_USER=inpi_user
DB_PASSWORD=votre_mot_de_passe_securise

# Configuration du serveur
PORT=3001
NODE_ENV=development

# JWT Secret (gÃ©nÃ©rer une clÃ© sÃ©curisÃ©e)
JWT_SECRET=votre_cle_jwt_super_secrete_changez_en_production

# Configuration du scraping
SCRAPING_DELAY=2000
MAX_CONCURRENT_REQUESTS=2
REQUEST_TIMEOUT=30000
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# Logs
LOG_LEVEL=info

# Rate limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100
```

### Ã‰tape 3: Initialiser la base de donnÃ©es
```bash
# ExÃ©cuter le script de configuration
npm run setup-db
```

**Vous devriez voir :**
```
ğŸ“¦ CrÃ©ation de la base de donnÃ©es...
ğŸ“‹ CrÃ©ation des tables...
ğŸ” CrÃ©ation des index...
ğŸ‘¤ CrÃ©ation de l'utilisateur de test...
âœ… Base de donnÃ©es configurÃ©e avec succÃ¨s !
ğŸ“§ Utilisateur de test crÃ©Ã© : test@example.com / test123
```

## ğŸš€ DÃ©marrage du Backend

### Ã‰tape 1: DÃ©marrer le serveur de dÃ©veloppement
```bash
npm run dev
```

**Vous devriez voir :**
```
âœ… Connexion Ã  la base de donnÃ©es rÃ©ussie
ğŸš€ Serveur dÃ©marrÃ© sur le port 3001
ğŸ“Š Dashboard: http://localhost:3001/api/health
ğŸ”§ Environment: development
```

### Ã‰tape 2: Tester l'API
```bash
# Test de santÃ© de l'API
curl http://localhost:3001/api/health

# Test de connexion Ã  la base de donnÃ©es
curl http://localhost:3001/api/test-db
```

## ğŸ”§ Configuration Frontend

### Ã‰tape 1: Mettre Ã  jour les variables d'environnement frontend
CrÃ©er un fichier `.env` dans le dossier frontend :
```env
REACT_APP_API_URL=http://localhost:3001/api
```

### Ã‰tape 2: RedÃ©marrer le frontend
```bash
# Dans le dossier frontend
npm run dev
```

## ğŸ§ª Tests et Validation

### 1. Test de l'authentification
```bash
# Inscription d'un nouvel utilisateur
curl -X POST http://localhost:3001/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "test123",
    "firstName": "Test",
    "lastName": "User"
  }'

# Connexion
curl -X POST http://localhost:3001/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "test123"
  }'
```

### 2. Test du scraping (avec token d'authentification)
```bash
# Remplacer YOUR_JWT_TOKEN par le token reÃ§u lors de la connexion
curl -X POST http://localhost:3001/api/scraping/start \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "apeCode": "0121Z",
    "department": "69",
    "siegeOnly": true
  }'
```

## ğŸ“Š Utilisation de l'Interface

### 1. AccÃ©der Ã  l'application
- Frontend : http://localhost:5173
- Backend API : http://localhost:3001

### 2. Se connecter
- Email : `test@example.com`
- Mot de passe : `test123`

### 3. Lancer un scraping
1. Aller dans "Nouveau Scraping"
2. Entrer un code APE (ex: 0121Z)
3. SÃ©lectionner un dÃ©partement (ex: 69)
4. Cliquer sur "Lancer le scraping"

### 4. Consulter les donnÃ©es
1. Aller dans "DonnÃ©es"
2. Voir les entreprises scrapÃ©es
3. Exporter en CSV si nÃ©cessaire

## ğŸ”’ SÃ©curitÃ© et Bonnes Pratiques

### 1. SÃ©curisation en production
```bash
# GÃ©nÃ©rer une clÃ© JWT sÃ©curisÃ©e
node -e "console.log(require('crypto').randomBytes(64).toString('hex'))"

# Utiliser HTTPS en production
# Configurer un reverse proxy (nginx)
# Utiliser des variables d'environnement sÃ©curisÃ©es
```

### 2. Monitoring et logs
```bash
# Consulter les logs
tail -f logs/combined.log
tail -f logs/error.log
```

### 3. Sauvegarde de la base de donnÃ©es
```bash
# Sauvegarde
mysqldump -u inpi_user -p inpi_scraper > backup_$(date +%Y%m%d).sql

# Restauration
mysql -u inpi_user -p inpi_scraper < backup_20240115.sql
```

## ğŸš¨ DÃ©pannage

### ProblÃ¨me : Erreur de connexion Ã  la base de donnÃ©es
```bash
# VÃ©rifier que MySQL est dÃ©marrÃ©
sudo systemctl status mysql

# VÃ©rifier les paramÃ¨tres de connexion
mysql -u inpi_user -p -h localhost inpi_scraper
```

### ProblÃ¨me : Erreur Puppeteer
```bash
# Installer les dÃ©pendances systÃ¨me (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wget
```

### ProblÃ¨me : Port dÃ©jÃ  utilisÃ©
```bash
# Trouver le processus utilisant le port 3001
lsof -i :3001

# Tuer le processus
kill -9 PID_DU_PROCESSUS
```

## ğŸ“ˆ Optimisations AvancÃ©es

### 1. Mise en cache avec Redis
```bash
# Installer Redis
sudo apt-get install redis-server

# DÃ©marrer Redis
sudo systemctl start redis-server
```

### 2. Queue de tÃ¢ches avec Bull
Le systÃ¨me de queue est dÃ©jÃ  configurÃ© pour gÃ©rer les tÃ¢ches de scraping en arriÃ¨re-plan.

### 3. Monitoring avec PM2 (production)
```bash
# Installer PM2
npm install -g pm2

# DÃ©marrer l'application
pm2 start src/server.js --name "inpi-scraper"

# Monitoring
pm2 monit
```

## âœ… Checklist de DÃ©ploiement

- [ ] Base de donnÃ©es configurÃ©e et accessible
- [ ] Variables d'environnement dÃ©finies
- [ ] Backend dÃ©marrÃ© sans erreurs
- [ ] Frontend connectÃ© au backend
- [ ] Tests d'authentification rÃ©ussis
- [ ] Test de scraping fonctionnel
- [ ] Logs configurÃ©s et accessibles
- [ ] Sauvegardes automatisÃ©es
- [ ] Monitoring en place
- [ ] SÃ©curitÃ© renforcÃ©e (HTTPS, rate limiting)

## ğŸ†˜ Support

En cas de problÃ¨me :
1. VÃ©rifiez les logs : `tail -f logs/combined.log`
2. Testez la connexion DB : `npm run setup-db`
3. VÃ©rifiez les ports : `lsof -i :3001`
4. Consultez la documentation des dÃ©pendances

---

**ğŸ‰ FÃ©licitations !** Votre backend INPI Scraper est maintenant opÃ©rationnel avec toutes les fonctionnalitÃ©s de scraping, authentification, et gestion des donnÃ©es.