# Guide d'installation - INPI Scraper

## PrÃ©requis

- Node.js v18+ installÃ©
- Compte Supabase (gratuit)
- Chrome/Chromium pour Puppeteer

## Ã‰tape 1: Configuration de Supabase

### 1.1 CrÃ©er les tables

1. Connectez-vous Ã  [Supabase](https://supabase.com)
2. Ouvrez votre projet
3. Allez dans **SQL Editor**
4. Copiez et exÃ©cutez le SQL suivant:

```sql
-- Table users
CREATE TABLE IF NOT EXISTS users (
  id uuid PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,
  email text UNIQUE NOT NULL,
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now()
);

-- Table companies
CREATE TABLE IF NOT EXISTS companies (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  denomination text NOT NULL,
  siren text NOT NULL,
  start_date date,
  representatives jsonb DEFAULT '[]'::jsonb,
  legal_form text,
  establishments integer DEFAULT 1,
  address text,
  postal_code text,
  city text,
  department text,
  ape_code text,
  status text DEFAULT 'active',
  scraped_at timestamptz DEFAULT now(),
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  UNIQUE(user_id, siren)
);

-- Table scraping_jobs
CREATE TABLE IF NOT EXISTS scraping_jobs (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id uuid NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  ape_code text,
  department text,
  siege_only boolean DEFAULT true,
  status text DEFAULT 'pending',
  progress integer DEFAULT 0,
  found_results integer DEFAULT 0,
  processed_results integer DEFAULT 0,
  error_message text,
  started_at timestamptz,
  completed_at timestamptz,
  created_at timestamptz DEFAULT now()
);

-- Index
CREATE INDEX IF NOT EXISTS idx_companies_user_id ON companies(user_id);
CREATE INDEX IF NOT EXISTS idx_companies_siren ON companies(siren);
CREATE INDEX IF NOT EXISTS idx_companies_department ON companies(department);
CREATE INDEX IF NOT EXISTS idx_companies_ape_code ON companies(ape_code);

-- RLS
ALTER TABLE users ENABLE ROW LEVEL SECURITY;
ALTER TABLE companies ENABLE ROW LEVEL SECURITY;
ALTER TABLE scraping_jobs ENABLE ROW LEVEL SECURITY;

-- Policies pour companies
CREATE POLICY "Users can view own companies"
  ON companies FOR SELECT TO authenticated
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own companies"
  ON companies FOR INSERT TO authenticated
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own companies"
  ON companies FOR UPDATE TO authenticated
  USING (auth.uid() = user_id)
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can delete own companies"
  ON companies FOR DELETE TO authenticated
  USING (auth.uid() = user_id);

-- Policies pour scraping_jobs
CREATE POLICY "Users can view own jobs"
  ON scraping_jobs FOR SELECT TO authenticated
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own jobs"
  ON scraping_jobs FOR INSERT TO authenticated
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own jobs"
  ON scraping_jobs FOR UPDATE TO authenticated
  USING (auth.uid() = user_id)
  WITH CHECK (auth.uid() = user_id);
```

### 1.2 DÃ©sactiver la confirmation d'email (pour le dÃ©veloppement)

1. Allez dans **Authentication** > **Settings**
2. DÃ©sactivez "Enable email confirmations"

## Ã‰tape 2: Configuration Backend

### 2.1 Installer les dÃ©pendances

```bash
cd backend
npm install
```

### 2.2 Configuration .env

Le fichier `backend/.env` est dÃ©jÃ  configurÃ© avec vos identifiants Supabase:

```env
PORT=3001
NODE_ENV=development
FRONTEND_URL=http://localhost:5173

SUPABASE_URL=https://0ec90b57d6e95fcbda19832f.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

SCRAPING_DELAY=2000
MAX_CONCURRENT_REQUESTS=3
REQUEST_TIMEOUT=30000
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
```

### 2.3 DÃ©marrer le backend

```bash
npm run dev
```

Vous devriez voir:
```
ğŸš€ Serveur dÃ©marrÃ© sur le port 3001
```

## Ã‰tape 3: Configuration Frontend

### 3.1 Installer les dÃ©pendances

```bash
cd ..  # Retour Ã  la racine
npm install
```

### 3.2 Configuration .env

Le fichier `.env` Ã  la racine est dÃ©jÃ  configurÃ©:

```env
VITE_SUPABASE_URL=https://0ec90b57d6e95fcbda19832f.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
VITE_API_URL=http://localhost:3001/api
```

### 3.3 DÃ©marrer le frontend

```bash
npm run dev
```

Le site s'ouvrira sur http://localhost:5173

## Ã‰tape 4: Tester l'application

### 4.1 VÃ©rifier le backend

1. Ouvrez http://localhost:3001/api/health
2. Vous devriez voir: `{"success":true,"message":"API INPI Scraper opÃ©rationnelle"}`

### 4.2 Utiliser l'application

L'application fonctionne en **mode dÃ©monstration** par dÃ©faut (sans authentification).

1. Ouvrez http://localhost:5173
2. Allez dans "Scraper"
3. Entrez un code APE (ex: 6201Z) et dÃ©partement (ex: 75)
4. Lancez le scraping

Les donnÃ©es seront stockÃ©es avec l'ID utilisateur `demo-user`.

## DÃ©pannage

### Erreur "Backend non disponible"

1. VÃ©rifiez que le backend tourne sur le port 3001
2. Testez http://localhost:3001/api/health
3. VÃ©rifiez les logs du backend dans le terminal

### Erreur "Cannot find module"

```bash
cd backend
rm -rf node_modules package-lock.json
npm install
```

### Erreur Supabase

1. VÃ©rifiez que les tables sont bien crÃ©Ã©es dans Supabase
2. VÃ©rifiez l'URL et la clÃ© Supabase dans le .env
3. VÃ©rifiez que RLS est activÃ© et les policies crÃ©Ã©es

### Erreur Puppeteer

Sur macOS/Linux:
```bash
cd backend/node_modules/puppeteer
npm run postinstall
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚  http://localhost:5173
â”‚  (React)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ API REST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend    â”‚  http://localhost:3001
â”‚  (Express)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚          â”‚
       â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Supabase â”‚  â”‚Puppeteer â”‚
â”‚   DB     â”‚  â”‚ Scraper  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## FonctionnalitÃ©s

- **Dashboard**: Statistiques et vue d'ensemble
- **Scraper**: Configuration et lancement de scraping
- **DonnÃ©es**: Gestion et recherche des entreprises
- **Export CSV**: Export des donnÃ©es sÃ©lectionnÃ©es
- **ParamÃ¨tres**: Configuration de l'application

## Notes importantes

- Le scraping INPI peut Ãªtre lent selon le nombre de rÃ©sultats
- En mode dÃ©mo, toutes les donnÃ©es sont associÃ©es Ã  l'utilisateur "demo-user"
- Pour activer l'authentification, implÃ©mentez le systÃ¨me d'auth Supabase dans le frontend
- Les donnÃ©es sont protÃ©gÃ©es par Row Level Security (RLS) en production